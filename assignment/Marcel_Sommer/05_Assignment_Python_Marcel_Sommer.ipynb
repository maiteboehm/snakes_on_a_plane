{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyG8nU1B7btq"
   },
   "source": [
    "# Assignment 05\n",
    "#### Python Basics V - Text Processing\n",
    "\n",
    "This tutorial was written by Terry L. Ruas (University of Göttingen). The references for external contributors for which this material was anyhow adapted/inspired are in the Acknowledgments section (end of the document)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjMwJ_zg7btr"
   },
   "source": [
    "This notebook will cover the following tasks:\n",
    "\n",
    "1. Text Pre-Processing\n",
    "2. Simple Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqcCFwVV7bts",
    "tags": []
   },
   "source": [
    "## Task 01 – Text Pre-Processing\n",
    "A computational analysis of natural language text typically requires several pre-processing steps, such as excluding irrelevant text parts, separating the text into words, phrases, or sentences depending on the analysis use case, removing so-called stop words, i.e., words that contain little to no semantic meaning, and normalizing the texts, e.g., by removing punctuation and capitalization.\n",
    "\n",
    "Use the *download_file()* function developed in the past assignments to download the plain text versions of Shakespeare’s play [Macbeth](https://ia802707.us.archive.org/1/items/macbeth02264gut/0ws3410.txt) and Bacon’s [New Atlantis](https://ia801309.us.archive.org/24/items/newatlantis02434gut/nwatl10.txt). If you choose not to implement assignment 4, task 6, download the files manually. We will also provide some txt files.\n",
    "\n",
    "Inspect these real-world texts manually to get an idea of what needs to be done to clean and prepare\n",
    "the texts for computational analysis. Implement the following functions to perform common pre-processing steps on the texts:\n",
    "1. *get_speaker_text()* – returns only the text spoken by the characters in the plays and removes all other text in the files, such as:\n",
    "    - Information about the file, acknowledgements, copyright notices etc.\n",
    "    - Headings indicating the act and scene\n",
    "    - Stage instructions\n",
    "    - Character names\n",
    "2. *normalize_text()*\n",
    "    - converts all text to lower case\n",
    "    - removes all punctuation from the texts\n",
    "3. *remove_stopwords()* – eliminates all stop words that are part of the list of English stop words (we provide two lists of stopwords, try both and see how they perform)\n",
    "4. *tokenize_text()* – splits the cleaned text into words\n",
    "\n",
    "This program is a pre-req for the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t5Mz4YP67btt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "\n",
    "\n",
    "path = str(os.getcwd())+ \"/Macbeth.txt\"\n",
    "path2 = str(os.getcwd())+ \"/assignment_05_program_data/eng_stop_words.txt\"\n",
    "\n",
    "\n",
    "def download_file(url, path):\n",
    "    if \".txt\" in url: \n",
    "        r = request.urlretrieve(url, path)\n",
    "        return print(\"Download successfull!\")\n",
    "    else: \n",
    "        return print(\"Download unsuccessfull!\")\n",
    "\n",
    "    \n",
    "def get_speaker_text(path):\n",
    "    '''opens the file and puts all lines in a list''' \n",
    "    with open(path, 'r') as f:\n",
    "            raw_text = f.readlines()\n",
    "    \n",
    "    '''puts all spoken text(everything that is indented or in the lines after indentation) in alist without name of speaker'''\n",
    "    spoken_text = []\n",
    "    counte = 0\n",
    "    for line in raw_text:\n",
    "        if raw_text.index(line) == 2:\n",
    "            counte = 0\n",
    "        elif line.startswith('  '):\n",
    "            counte = 1\n",
    "            splited_line=line.split('.')\n",
    "            spoken_text.append(''.join(splited_line[1:]))\n",
    "        elif counte == 1:\n",
    "            spoken_text.append(line)\n",
    "        else:\n",
    "            continue\n",
    "                \n",
    "    '''puts all lines after the start_index in a list '''\n",
    "    drama_text = []    \n",
    "    start_index = spoken_text.index(' When shall we three meet againe?\\n')   \n",
    "    for line in spoken_text:\n",
    "        if spoken_text.index(line) >= start_index:\n",
    "            drama_text.append(line)\n",
    "           \n",
    "    '''puts all lines without brackets in a list '''        \n",
    "    speaker_text = []\n",
    "    counter = 0\n",
    "    for line in drama_text:\n",
    "        if counter == 1 and not ')'in line:\n",
    "            continue \n",
    "            \n",
    "        elif '('in line and ')'in line:\n",
    "            words_in_line=line.split()\n",
    "            conter = 0\n",
    "            words =[]\n",
    "            for word in words_in_line:\n",
    "                if conter ==1 and ')' not in word:\n",
    "                    continue\n",
    "                elif '(' in word:\n",
    "                    conter=1\n",
    "                elif ')' in word:\n",
    "                    conter=0    \n",
    "                else:\n",
    "                    words.append(word)\n",
    "            words.append('\\n')        \n",
    "            speaker_text.append(' '.join(words)) \n",
    "            \n",
    "        elif '(' in line:\n",
    "            counter = 1\n",
    "            words_in_line=line.split()\n",
    "            conter = 0\n",
    "            words =[]\n",
    "            for word in words_in_line:\n",
    "                if '(' in word:\n",
    "                    conter=1\n",
    "                elif conter ==1:\n",
    "                    continue\n",
    "                else:\n",
    "                    words.append(word)        \n",
    "            speaker_text.append(' '.join(words))\n",
    "                        \n",
    "        elif ')' in line:\n",
    "            counter = 0\n",
    "            words_in_line=line.split()\n",
    "            conter = 1\n",
    "            words =[]\n",
    "            for word in words_in_line:\n",
    "                if ')' in word:\n",
    "                    conter=0\n",
    "                elif conter ==1:\n",
    "                    continue\n",
    "                else:\n",
    "                    words.append(word)\n",
    "            words.append('\\n')         \n",
    "            speaker_text.append(' '.join(words))\n",
    "        else:\n",
    "            speaker_text.append(line)  \n",
    "    return speaker_text\n",
    "''' removes special character from the lines of a list '''\n",
    "def normalize_text(input_text):\n",
    "    normalized_text=[]\n",
    "    for line in input_text:\n",
    "        lower_case_line = line.lower()\n",
    "        lower_case_line = lower_case_line.replace(',', '')\n",
    "        lower_case_line = lower_case_line.replace('&', '')\n",
    "        lower_case_line = lower_case_line.replace(';', '')\n",
    "        lower_case_line = lower_case_line.replace('.', '')\n",
    "        lower_case_line = lower_case_line.replace('?', '')\n",
    "        lower_case_line = lower_case_line.replace('!', '')\n",
    "        lower_case_line = lower_case_line.replace(':', '')\n",
    "        lower_case_line = lower_case_line.replace('\\'', ' ')\n",
    "        lower_case_line = lower_case_line.replace('\\n', '')\n",
    "        normalized_text.append(lower_case_line)\n",
    "    normalized_text = ' '.join(normalized_text)    \n",
    "    return normalized_text\n",
    "\n",
    "def correct_ocr_errors(text):\n",
    "    #  vowels {V}\n",
    "    vowel = {\"a\", \"e\", \"i\", \"o\", \"y\"}\n",
    "    #  consonants {C}\n",
    "    cons = {\"q\", \"w\", \"r\", \"t\", \"p\", \"s\", \"d\", \"f\", \"h\", \"j\", \"k\", \"l\", \"c\", \"x\", \"b\", \"n\", \"m\"}\n",
    "\n",
    "    # 'd -> ed\n",
    "    text = re.sub(\"\\'d\", \"ed\", text)\n",
    "\n",
    "    chars = list(text)\n",
    "    for i in range(len(chars)):\n",
    "        # v{C} -> u\n",
    "        if chars[i] == \"v\" and chars[i + 1] in cons:\n",
    "            chars[i] = \"u\"\n",
    "            i += 1\n",
    "        # {C,V}u{V} -> v\n",
    "        if chars[i] == \"u\" and chars[i + 1] in vowel and chars[i - 1] != \"q\":\n",
    "            chars[i] = \"v\"\n",
    "            i += 1\n",
    "        # {C}ne -> {C}n in verbs and some nouns\n",
    "        if chars[i] == \" \" and chars[i - 1] == \"e\" and chars[i - 2] == \"n\" and chars[i - 3] in cons:\n",
    "            chars[i - 1] = \"\"\n",
    "    # .*esse plural in nouns and adjectives (and some infinitive forms)\n",
    "    text = \"\".join(chars)\n",
    "    text = re.sub(\"esse \", \"ess \", text)\n",
    "    # {C}ye -> {C}ie\n",
    "    text = re.sub(\"ie\\s\", \"y \", text)\n",
    "    # ie -> y at ending\n",
    "    text = re.sub(\"ie \", \"y \", text)\n",
    "    # io -> j at beginning\n",
    "    text = re.sub(\" iu\", \" ju\", text)\n",
    "    return text\n",
    "\n",
    "''' remove english stopwords from a txt-file and puts all words in a string'''\n",
    "def remove_stopwords(input_text, path2):\n",
    "    clear_text =[]\n",
    "    stop_word_list=[]\n",
    "    stop_word_clear=[]\n",
    "    with open(path2, 'r') as f:\n",
    "            stop_words = f.readlines()   \n",
    "    for line in stop_words:\n",
    "        stop_word_list.append(line)\n",
    "    for line in stop_word_list:    \n",
    "        clear_line=line.replace('\\n','')\n",
    "        stop_word_clear.append(clear_line)\n",
    "        \n",
    "    for st_word in stop_word_clear:\n",
    "        word_to_clear = ' '+ st_word + ' '\n",
    "        input_text =  input_text.replace(word_to_clear,' ')       \n",
    "    return input_text   \n",
    "\n",
    "''' trasfers string in to list'''\n",
    "def tokenize_text(input_string):\n",
    "    output_list = input_string.split()\n",
    "    return output_list\n",
    "\n",
    "#download_file(\"https://ia802707.us.archive.org/1/items/macbeth02264gut/0ws3410.txt\", path)\n",
    "speaker_text=get_speaker_text(path)\n",
    "normalized_text=normalize_text(speaker_text)\n",
    "corrected_text=correct_ocr_errors(normalized_text)\n",
    "clear_text=remove_stopwords(corrected_text, path2)\n",
    "output_word_list_macbeth= tokenize_text(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcpnC_hq7btu"
   },
   "source": [
    "## Task 02 – Classes\n",
    "The [Baconian theory](https://en.wikipedia.org/wiki/Baconian_theory_of_Shakespeare_authorship) holds that Sir Francis Bacon is the author of Shakespeare’s plays. We want to perform a very simple stylistic analysis between Shakespeare’s play Macbeth and Bacon’s New Atlantis. We check for words that frequently occur in both documents to see whether there are characteristic words that co-occur in the texts, which might give some support to the theory.\n",
    "\n",
    "Your Task:\n",
    "1. Download and pre-process the texts as follows:  \n",
    "  New Atlantis\n",
    "    1. *get_speaker_text()*\n",
    "    2. *normalize_text()*\n",
    "    3. *remove_stopwords()*\n",
    "    4. *tokenize_text()*   \n",
    "  \n",
    " Macbeth\n",
    "    1. *get_speaker_text()*\n",
    "    2. *normalize_text()*\n",
    "        1. *utils_ocr.correct_ocr_errors()* – we will provide a function to deal with OCR errors.\n",
    "    3. *remove_stopwords()*\n",
    "    4. *tokenize_text()*\n",
    "2. For the pre-processed texts, compute the list of word co-occurrence frequencies, i.e. which words occur in both documents and how often. Use the format:  \n",
    "[term , frequency_doc1 , frequency_doc2 , sum_of_frequencies]  \n",
    "Sort the list according to the sum of the frequencies in descending order.\n",
    "3. Use the csv library to store the ordered word co-occurrence frequency list in a CSV file. **You can zip the csv and upload it to GitHub.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LPkEibMM7btu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "path1 = str(os.getcwd())+ \"/NewAtlatis.txt\"\n",
    "path2 = str(os.getcwd())+ \"/assignment_05_program_data/eng_stop_words.txt\"\n",
    "\n",
    "\n",
    "def download_file(url, path1):\n",
    "    if \".txt\" in url: \n",
    "        r = request.urlretrieve(url, path1)\n",
    "        return print(\"Download successfull!\")\n",
    "    else: \n",
    "        return print(\"Download unsuccessfull!\")\n",
    "\n",
    "def get_speaker_text_new(path1):\n",
    "    '''opens the file and puts all lines in a list'''\n",
    "    speaker_text=[] \n",
    "    with open(path1, 'r') as f:\n",
    "            raw_text = f.readlines()       \n",
    "    start_index = raw_text.index('We sailed from Peru, (where we had continued for the space of one\\n')\n",
    "    for line in raw_text:\n",
    "        if raw_text.index(line) >= start_index:\n",
    "            speaker_text.append(line)        \n",
    "    return speaker_text    \n",
    "\n",
    "def normalize_text_new(input_text):\n",
    "    normalized_text=[]\n",
    "    for line in input_text:\n",
    "        lower_case_line = line.lower()\n",
    "        lower_case_line = lower_case_line.replace(',', '')\n",
    "        lower_case_line = lower_case_line.replace('(', '')\n",
    "        lower_case_line = lower_case_line.replace(')', '')\n",
    "        lower_case_line = lower_case_line.replace('&', '')\n",
    "        lower_case_line = lower_case_line.replace(';', '')\n",
    "        lower_case_line = lower_case_line.replace('.', '')\n",
    "        lower_case_line = lower_case_line.replace('?', '')\n",
    "        lower_case_line = lower_case_line.replace('!', '')\n",
    "        lower_case_line = lower_case_line.replace(':', '')\n",
    "        lower_case_line = lower_case_line.replace('\\'', ' ')\n",
    "        lower_case_line = lower_case_line.replace('\\n', '')\n",
    "        normalized_text.append(lower_case_line)\n",
    "    return normalized_text\n",
    "\n",
    "''' remove english stopwords from a txt-file and puts all words in a string'''\n",
    "def remove_stopwords(input_text, path2):\n",
    "    clear_text =[]\n",
    "    stop_word_list=[]\n",
    "    stop_word_clear=[]\n",
    "    with open(path2, 'r') as f:\n",
    "            stop_words = f.readlines()   \n",
    "    for line in stop_words:\n",
    "        stop_word_list.append(line)\n",
    "    for line in stop_word_list:    \n",
    "        clear_line=line.replace('\\n','')\n",
    "        stop_word_clear.append(clear_line)\n",
    "        \n",
    "    input_text = ' '.join(input_text)\n",
    "\n",
    "    for st_word in stop_word_clear:\n",
    "        word_to_clear = ' '+ st_word + ' '\n",
    "        input_text =  input_text.replace(word_to_clear,' ')       \n",
    "    return input_text   \n",
    "\n",
    "''' trasfers string in to list'''\n",
    "def tokenize_text(input_string):\n",
    "    output_list = input_string.split()\n",
    "    return output_list\n",
    "\n",
    "\n",
    "#download_file('https://ia801309.us.archive.org/24/items/newatlantis02434gut/nwatl10.txt', path1)\n",
    "speaker_text_new=get_speaker_text_new(path1)\n",
    "normalized_text_new=normalize_text_new(speaker_text_new)\n",
    "clear_text_new=remove_stopwords(normalized_text_new, path2)\n",
    "output_word_list_atlatis= tokenize_text(clear_text_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import csv\n",
    "\n",
    "\n",
    "macbeth = output_word_list_macbeth\n",
    "new_atlantis = output_word_list_atlatis\n",
    "\n",
    "output_list = []\n",
    "word_list = []\n",
    "\n",
    "for word in macbeth:\n",
    "    output = []\n",
    "    if word not in word_list:\n",
    "        word_list.append(word)\n",
    "        count_m = macbeth.count(word)\n",
    "        count_a = new_atlantis.count(word)\n",
    "        output.append(word)\n",
    "        output.append(count_m)\n",
    "        output.append(count_a)\n",
    "        output.append(count_m + count_a)\n",
    "        output_list.append(output)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "for word in new_atlantis:\n",
    "    output = []\n",
    "    if word not in word_list:\n",
    "        word_list.append(word)\n",
    "        count_m = macbeth.count(word)\n",
    "        count_a = new_atlantis.count(word)\n",
    "        output.append(word)\n",
    "        output.append(count_m)\n",
    "        output.append(count_a)\n",
    "        output.append(count_m + count_a)\n",
    "        output_list.append(output)\n",
    "\n",
    "output_list = sorted(output_list, key=itemgetter(3), reverse=True)       \n",
    "#print(output_list)\n",
    "\n",
    "with open('baconian_theory.csv', mode='w') as file:\n",
    "    fieldnames = ['Word', 'Frequency Macbeth', 'Frequency New Atlantis', 'Sum']\n",
    "    file_writer = csv.writer(file, delimiter=' ')\n",
    "    file_writer.writerow(['word', 'frequencyM', 'frequencyNA', 'sum'])\n",
    "    \n",
    "    for line in output_list:\n",
    "        file_writer.writerow(line)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
